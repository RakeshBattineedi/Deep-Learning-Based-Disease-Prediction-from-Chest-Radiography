{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport horovod.tensorflow.keras as hvd\nimport os\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm import tqdm\nfrom itertools import chain\nimport matplotlib.pyplot as plt\n\n# Horovod: initialize Horovod.\nhvd.init()\n\n# Horovod: pin GPU to be used to process local rank (one GPU per process)\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)\nif gpus:\n    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n\ntotal_partitions = hvd.size()\n\n\n#Reading the target labels\ndf = pd.read_csv('./sample/sample_labels.csv', index_col= False)\ndf.index.astype('int64')\ntotal_rows = df.shape[0]\nbatch_size = total_rows//total_partitions\n\nfor partition in range(total_partitions):\n    if hvd.rank() == partition:\n        start = partition * batch_size\n        end = (partition+1) * batch_size\n        df = df.iloc[start:end-1, :]\n\ndf['Finding Labels'] = df['Finding Labels'].map(lambda x: x.replace('No Finding', ''))\nall_labels = np.unique(list(chain(*df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n\nall_labels = [x for x in all_labels if len(x)>0]\n\nfor c_label in all_labels:\n    if len(c_label)>1: # leave out empty labels\n        df[c_label] = df['Finding Labels'].map(lambda finding: 1.0 if c_label in finding else 0)\n\ndf['disease_vec'] = df.apply(lambda x: [x[all_labels].values], 1).map(lambda x: x[0])\ny=df['disease_vec']\n\n# Reading Images into arrays\ntrain_path = './sample/sample/images/'\ndata = os.listdir(train_path)\nimg_size =100\ntrain_data=[]\ni =0\nfor partition in range(total_partitions):\n    if hvd.rank() == partition:\n        start = partition * batch_size\n        end = (partition+1) * batch_size\n        data= data[start:end-1]\nfor img in tqdm(data):\n    img_array = cv2.imread(os.path.join(train_path,img))\n    new_array = cv2.resize(img_array,(img_size,img_size))\n    train_data.append([new_array,y.iloc[i]])\n    i= i+1\n\nX=[]\nY=[]\n\nfor i,j in train_data:\n    X.append(i)\n    Y.append(j)\n\nX = np.array(X)\nX =X/255.0\nY = np.array(Y).astype(np.float32)\nindx = int(0.9*X.shape[0])\ntrainX, testX = X[:indx,:], X[indx:,:]\ntrainY, testY = Y[:indx,:], Y[indx:,:]\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(256, (3, 3), input_shape=X.shape[1:]),\n    tf.keras.layers.Conv2D(filters=96, kernel_size=(11, 11),strides = 3, padding= 'same', input_shape =(128,128,1),\n                 kernel_regularizer=l2(0.)),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Conv2D(64, (3, 3),padding= 'same', activation='relu'),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Dropout(0.25),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(14),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Activation('sigmoid')\n])\n\n# Horovod: adjust learning rate based on number of GPUs.\nopt = tf.optimizers.Adam(0.001 * hvd.size())\n\n# Horovod: add Horovod DistributedOptimizer.\nopt = hvd.DistributedOptimizer(opt)\n\n# Horovod: Specify `experimental_run_tf_function=False` to ensure TensorFlow\n# uses hvd.DistributedOptimizer() to compute gradients.\nmodel.compile(loss=tf.losses.BinaryCrossentropy(), optimizer=opt, metrics=['accuracy'],\n                    experimental_run_tf_function=False)\n\ncallbacks = [\n    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n    # This is necessary to ensure consistent initialization of all workers when\n    # training is started with random weights or restored from a checkpoint.\n    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n\n    # Horovod: average metrics among workers at the end of every epoch.\n    #\n    # Note: This callback must be in the list before the ReduceLROnPlateau,\n    # TensorBoard or other metrics-based callbacks.\n    hvd.callbacks.MetricAverageCallback(),\n\n    # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final\n    # accuracy. Scale the learning rate `lr = 1.0` ---> `lr = 1.0 * hvd.size()` during\n    # the first three epochs. See https://arxiv.org/abs/1706.02677 for details.\n    hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=3, verbose=1),\n]\n\n# Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n#if hvd.rank() == 0:\n#    callbacks.append(tf.keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}.h5'))\n\n# Horovod: write logs on worker 0.\nverbose = 1 if hvd.rank() == 0 else 0\n\n# Train the model.\n# Horovod: adjust number of steps based on number of GPUs.\nhistory = model.fit(x= trainX, y = trainY, batch_size = 16, validation_data=(testX, testY),callbacks=callbacks,\n                    epochs=5, verbose=verbose)\n\n_, train_acc = model.evaluate(trainX, trainY, verbose=0)\n_, test_acc = model.evaluate(testX, testY, verbose=0)\n\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n\n# learning curves of model accuracy\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
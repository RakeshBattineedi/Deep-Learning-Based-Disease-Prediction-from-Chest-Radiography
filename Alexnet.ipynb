{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport horovod.tensorflow.keras as hvd\nimport os\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm import tqdm\nfrom itertools import chain\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Horovod: initialize Horovod.\nhvd.init()\n\n# Horovod: pin GPU to be used to process local rank (one GPU per process)\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth(gpu, True)\nif gpus:\n    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n\ntotal_partitions = hvd.size()\n\n\n#Reading the target labels\ndf = pd.read_csv('./sample/sample_labels.csv', index_col= False)\ndf.index.astype('int64')\ntotal_rows = df.shape[0]\nbatch_size = total_rows//total_partitions\n\nfor partition in range(total_partitions):\n    if hvd.rank() == partition:\n        start = partition * batch_size\n        end = (partition+1) * batch_size\n        df = df.iloc[start:end-1, :]\n\nall_labels = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'Nodule', 'Pleural_Thickening',\n 'Pneumonia', 'Pneumothorax','No Finding']\n\n#Making the labels as column and assigning 1 if it exists in Finding Column else 0\nfor c_label in all_labels:\n    if len(c_label)>1:\n        df[c_label] = df['Finding Labels'].map(lambda finding: 1.0 if c_label in finding else 0)\n\n# Reading Images into arrays\ntrain_path = './sample/sample/images/'\ndata = os.listdir(train_path)\ntrain_data=[]\nimagenames_list = []\ni =0\n\nfor partition in range(total_partitions):\n    if hvd.rank() == partition:\n        start = partition * batch_size\n        end = (partition+1) * batch_size\n        data= data[start:end-1]\nfor img in tqdm(data):\n    img_array = os.path.join(train_path,img)\n    imagenames_list.append(img_array)\n\ndf['path'] = imagenames_list\n\ntrain_df, test_df = train_test_split(df,test_size = 0.20,random_state = 2018)\n\nIMG_SIZE = (100, 100)\ndatagen = ImageDataGenerator(samplewise_center=True,\n                              samplewise_std_normalization=True,\n                              horizontal_flip = True,\n                              vertical_flip = False,\n                              height_shift_range= 0.05,\n                              width_shift_range=0.1,\n                              rotation_range=5,\n                              shear_range = 0.1,\n                              fill_mode = 'reflect',\n                              zoom_range=0.15)\n\ntrain_generator = datagen.flow_from_dataframe(\n            dataframe=train_df,\n            directory=None,\n            x_col=\"path\",\n            y_col= ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'Nodule', 'Pleural_Thickening',\n 'Pneumonia', 'Pneumothorax','No Finding'],\n            subset=\"training\",\n            batch_size=256,\n            seed=42,\n            shuffle=True,\n            target_size= IMG_SIZE,rescale=1.0/255.0, color_mode='grayscale',class_mode = 'raw')\n\ntest_datagen=ImageDataGenerator(rescale=1./255.)\ntest_generator = test_datagen.flow_from_dataframe(\ndataframe=test_df,\ndirectory=None,\nx_col=\"path\",\ny_col=['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'Nodule', 'Pleural_Thickening',\n 'Pneumonia', 'Pneumothorax', 'No Finding'],\nclass_mode=\"raw\",\nbatch_size=1024,\nseed=42,\nshuffle=False,\ntarget_size=IMG_SIZE,rescale=1.0/255.0, color_mode='grayscale')\n\nmodel = tf.keras.Sequential([\ntf.keras.layersConv2D(filters=96, kernel_size=(11, 11),input_shape =(128,128,1),strides=4, kernel_regularizer=l2(0.)),\ntf.keras.layers.Activation('relu'),\ntf.keras.layers.MaxPooling2D(pool_size=(3, 3)),\ntf.keras.layers.BatchNormalization(),\n\n#Layer 2\ntf.keras.layers.Conv2D(256, (5, 5), padding='same'),\ntf.keras.layers.Activation('relu'),\ntf.keras.layers.MaxPooling2D(pool_size=(3, 3)),\ntf.keras.layers.BatchNormalization(),\n\n# Layer 3\ntf.keras.layers.Conv2D(384, (3, 3), padding='same'),\ntf.keras.layers.Activation('relu'),\n\n#Layer 4\ntf.keras.layers.Conv2D(384, (3, 3), padding='same'),\ntf.keras.layers.Activation('relu'),\n\n# Layer 5\ntf.keras.layers.Conv2D(256, (3, 3), padding='same'),\ntf.keras.layers.Activation('relu'),\ntf.keras.layers.MaxPooling2D(pool_size=(3, 3)),\n\n# Layer 6\ntf.keras.layers.Flatten(),\ntf.keras.layers.Dense(4096),\ntf.keras.layers.Activation('relu'),\ntf.keras.layers.Dropout(0.5),\n\n# Layer 7\ntf.keras.layers.Dense(4096),\ntf.keras.layers.Activation('relu'),\ntf.keras.layers.Dropout(0.5),\n\n# Layer 8\ntf.keras.layers.Dense(15, activation='softmax')\n])\n\n# Horovod: adjust learning rate based on number of GPUs.\nopt = tf.optimizers.Adam(0.001 * hvd.size())\n\n# Horovod: add Horovod DistributedOptimizer.\nopt = hvd.DistributedOptimizer(opt)\n\n# Horovod: Specify `experimental_run_tf_function=False` to ensure TensorFlow\n# uses hvd.DistributedOptimizer() to compute gradients.\nmodel.compile(loss=tf.losses.BinaryCrossentropy(), optimizer=opt, metrics=['accuracy'],\n                    experimental_run_tf_function=False)\n\ncallbacks = [\n    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n    # This is necessary to ensure consistent initialization of all workers when\n    # training is started with random weights or restored from a checkpoint.\n    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n\n    # Horovod: average metrics among workers at the end of every epoch.\n    #\n    # Note: This callback must be in the list before the ReduceLROnPlateau,\n    # TensorBoard or other metrics-based callbacks.\n    hvd.callbacks.MetricAverageCallback(),\n\n    # Horovod: using `lr = 1.0 * hvd.size()` from the very beginning leads to worse final\n    # accuracy. Scale the learning rate `lr = 1.0` ---> `lr = 1.0 * hvd.size()` during\n    # the first three epochs. See https://arxiv.org/abs/1706.02677 for details.\n    hvd.callbacks.LearningRateWarmupCallback(warmup_epochs=3, verbose=1),\n]\n\n# Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n#if hvd.rank() == 0:\n#    callbacks.append(tf.keras.callbacks.ModelCheckpoint('./checkpoint-{epoch}.h5'))\n\n# Horovod: write logs on worker 0.\nverbose = 1 if hvd.rank() == 0 else 0\n\n# Train the model.\n# Horovod: adjust number of steps based on number of GPUs.\nhistory = history = model.fit_generator(train_generator, steps_per_epoch=len(train_generator),\n                                        validation_data=test_generator,epochs=10, verbose=1)\n\n(loss, accuracy) = model.evaluate_generator(test_generator,verbose=1)\nprint('[INFO] accuracy: {:.2f}%'.format(accuracy * 100))\n\npredicted = model.predict_generator(test_generator, steps = len(test_generator), verbose = True)\n\nfrom matplotlib import pyplot as plt\ndef summarize_diagnostics(history):\n    figure,ax = plt.subplots()\n    plt.figure(figsize=(10,10))\n# plot loss\n    plt.subplot(211)\n    plt.title('Cross Entropy Loss')\n    plt.plot(history.history['loss'], color='blue', label='train')\n    plt.plot(history.history['val_loss'], color='orange', label='test')\n    # plot accuracy\n    plt.subplot(212)\n    plt.title('Accuracy')\n    plt.plot(history.history['accuracy'], color='blue', label='train')\n    plt.plot(history.history['val_accuracy'], color='orange', label='test')\n    figure.tight_layout(pad=3.0)\n\nsummarize_diagnostics(history)\n\ntest_labels = []\nfor i in range(0,5):\n    test_labels.extend(np.array(test_generator[i][1]))\nlen(test_labels)\n\n\ntest_x,test_y = next(test_generator)\n\nfor c_label, p_count, t_count in zip(all_labels, \n                                     100*np.mean(test_labels,0), \n                                     100*np.mean(predicted,0)):\n    print('%s: Dx: %2.2f%%, PDx: %2.2f%%' % (c_label, t_count, p_count))\n    \n    \n    \nfrom sklearn.metrics import roc_curve, auc\nfig, c_ax = plt.subplots(1,1, figsize = (9, 9))\nfor (idx, c_label) in enumerate(all_labels):\n    fpr, tpr, thresholds = roc_curve(test_labels[:,idx].astype(int), predicted[:,idx])\n    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\nc_ax.legend()\nc_ax.set_xlabel('False Positive Rate')\nc_ax.set_ylabel('True Positive Rate')\nfig.savefig('barely_trained_net.png')\n\n\nsickest_idx = np.argsort(np.sum(test_y, 1)<1)\nfig, m_axs = plt.subplots(4, 3, figsize = (10, 15))\nfor (idx, c_ax) in zip(sickest_idx, m_axs.flatten()):\n    c_ax.imshow(test_x[idx, :,:,0], cmap = 'bone')\n    stat_str = [n_class[:6] for n_class, n_score in zip(all_labels, \n                                                                  test_labels[idx]) \n                             if n_score>0.5]\n    pred_str = ['%s:%2.0f%%' % (n_class[:4], p_score*100)  for n_class, n_score, p_score in zip(all_labels, \n                                                                  test_labels[idx], predicted[idx]) \n                             if (n_score>0.5) or (p_score>0.5)]\n    c_ax.set_title('Dx: '+', '.join(stat_str)+'\\nPDx: '+', '.join(pred_str))\n    c_ax.axis('off')\nfig.savefig('trained_img_predictions.png')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}